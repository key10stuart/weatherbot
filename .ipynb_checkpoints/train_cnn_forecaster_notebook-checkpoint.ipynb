{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e302ce",
   "metadata": {},
   "source": [
    "\n",
    "# Dilated Causal CNN Forecaster — Notebook Version\n",
    "\n",
    "This notebook refactors your `train_cnn_forecaster.py` script into modular, commented cells so you can iterate, test, and refine interactively.  \n",
    "**Workflow**: Configure → Load/Prep Data → Build Model → Train → Evaluate → Save.\n",
    "\n",
    "> Notes:\n",
    "> - Replace argparse with a `CFG` dict for parameters.\n",
    "> - Uses MPS on Apple Silicon if available, otherwise CPU.\n",
    "> - Huber loss + AdamW + OneCycle. Early stopping on val RMSE.\n",
    "> - Metrics include per-horizon RMSE and **skill vs. persistence**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611f91a",
   "metadata": {},
   "source": [
    "## Imports & Global Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca54f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math, os, sys, time, json, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(s: int = 42):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce5f3a",
   "metadata": {},
   "source": [
    "## Data Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707fb1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _squeeze_last_if_singleton(a: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"If target is (N, H, 1) or (N, 1), drop the last dim.\"\"\"\n",
    "    return a[..., 0] if (a.ndim >= 1 and a.shape[-1] == 1) else a\n",
    "\n",
    "def load_npz(path: str):\n",
    "    \"\"\"Load NPZ with keys: X_train, y_train, X_val, y_val, X_test, y_test.\"\"\"\n",
    "    npz = np.load(path)\n",
    "    X_train = npz[\"X_train\"]\n",
    "    y_train = _squeeze_last_if_singleton(npz[\"y_train\"])\n",
    "    X_val   = npz[\"X_val\"]\n",
    "    y_val   = _squeeze_last_if_singleton(npz[\"y_val\"])\n",
    "    X_test  = npz[\"X_test\"]\n",
    "    y_test  = _squeeze_last_if_singleton(npz[\"y_test\"])\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905aa172",
   "metadata": {},
   "source": [
    "## Normalization & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24509eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zscore_fit(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute per-feature μ/σ from TRAIN windows only. X: [N, L, F].\"\"\"\n",
    "    mu = X.reshape(-1, X.shape[-1]).mean(axis=0)\n",
    "    sd = X.reshape(-1, X.shape[-1]).std(axis=0)\n",
    "    sd = np.where(sd < 1e-8, 1.0, sd)\n",
    "    return mu.astype(np.float32), sd.astype(np.float32)\n",
    "\n",
    "def zscore_apply(X: np.ndarray, mu: np.ndarray, sd: np.ndarray) -> np.ndarray:\n",
    "    return (X - mu) / sd\n",
    "\n",
    "class TSWindowDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.as_tensor(X, dtype=torch.float32)  # [N,L,F]\n",
    "        self.y = torch.as_tensor(y, dtype=torch.float32)  # [N,H]\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32743625",
   "metadata": {},
   "source": [
    "## Device Selection & Autotuning Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4210c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pick_device():\n",
    "    \"\"\"Prefer MPS (Apple Silicon), else CPU (no CUDA here).\"\"\"\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def autotune_hparams(CFG, device):\n",
    "    \"\"\"\n",
    "    Heuristics:\n",
    "      - MPS (Apple Silicon): keep defaults, bump workers a bit.\n",
    "      - CPU (assume ~16 GB RAM): clamp to a lighter profile.\n",
    "    \"\"\"\n",
    "    tuned = dict(CFG)\n",
    "    if device.type == \"mps\":\n",
    "        tuned[\"num_workers\"] = max(tuned.get(\"num_workers\", 2), 4)\n",
    "        return tuned\n",
    "\n",
    "    tuned[\"batch_size\"] = min(CFG.get(\"batch_size\", 128), 64)\n",
    "    tuned[\"channels\"]   = min(CFG.get(\"channels\", 64), 48)\n",
    "    tuned[\"epochs\"]     = min(CFG.get(\"epochs\", 80), 80)\n",
    "    tuned[\"num_workers\"] = max(CFG.get(\"num_workers\", 2), 2)\n",
    "    tuned[\"lr\"]         = min(CFG.get(\"lr\", 3e-3), 2e-3)\n",
    "    return tuned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee5e0c",
   "metadata": {},
   "source": [
    "## Model: Dilated Causal CNN with Residual Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f2d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CausalConv1d(nn.Conv1d):\n",
    "    \"\"\"1D convolution with left-padding only (causal; no future leakage).\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k, dilation=1):\n",
    "        super().__init__(in_ch, out_ch, kernel_size=k, dilation=dilation, padding=0, bias=True)\n",
    "        self.left_pad = (k - 1) * dilation\n",
    "    def forward(self, x):  # x: [B,C,T]\n",
    "        if self.left_pad > 0:\n",
    "            x = torch.nn.functional.pad(x, (self.left_pad, 0))\n",
    "        return super().forward(x)\n",
    "\n",
    "class LayerNormChannel(nn.Module):\n",
    "    \"\"\"LayerNorm over channel dim at each timestep (expects input [B,C,T]).\"\"\"\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(C)\n",
    "    def forward(self, x):  # x: [B,C,T]\n",
    "        x = x.transpose(1, 2).contiguous()   # [B,T,C]\n",
    "        x = self.ln(x)\n",
    "        x = x.transpose(1, 2).contiguous()   # back to [B,C,T]\n",
    "        return x\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, C, k, dilation, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = CausalConv1d(C, C, k, dilation)\n",
    "        self.act1  = nn.GELU()\n",
    "        self.norm1 = LayerNormChannel(C)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = CausalConv1d(C, C, k, dilation)\n",
    "        self.act2  = nn.GELU()\n",
    "        self.norm2 = LayerNormChannel(C)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):  # x: [B,C,T]\n",
    "        residual = x\n",
    "        x = self.conv1(x); x = self.act1(x); x = self.norm1(x); x = self.drop1(x)\n",
    "        x = self.conv2(x); x = self.act2(x); x = self.norm2(x); x = self.drop2(x)\n",
    "        return x + residual  # residual connection\n",
    "\n",
    "class DilatedCausalCNN(nn.Module):\n",
    "    def __init__(self, in_feats, C=64, k=5, dilations=(1,2,4,8,16,32), horizon=24, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.in_proj = nn.Conv1d(in_feats, C, kernel_size=1)\n",
    "        self.blocks = nn.ModuleList([ResBlock(C, k=k, dilation=d, dropout=dropout) for d in dilations])\n",
    "        self.head_norm = LayerNormChannel(C)\n",
    "        self.head = nn.Linear(C, horizon)  # last-timestep embedding -> H outputs\n",
    "\n",
    "    def forward(self, x):  # x: [B,L,F]\n",
    "        x = x.transpose(1, 2).contiguous()  # [B,F,L]\n",
    "        x = self.in_proj(x)                 # [B,C,L]\n",
    "        for b in self.blocks:\n",
    "            x = b(x)                        # [B,C,L]\n",
    "        x = self.head_norm(x)               # [B,C,L]\n",
    "        last = x[:, :, -1].contiguous()     # [B,C]\n",
    "        yhat = self.head(last)              # [B,H]\n",
    "        return yhat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed107261",
   "metadata": {},
   "source": [
    "## Metrics & Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rmse(a, b, dim=None, eps=1e-9):\n",
    "    return torch.sqrt(torch.mean((a - b)**2, dim=dim) + eps)\n",
    "\n",
    "def persistence_baseline(x_last, H):\n",
    "    \"\"\"Repeat last observed target H times.\"\"\"\n",
    "    return x_last.unsqueeze(1).repeat(1, H)\n",
    "\n",
    "def per_horizon_rmse(yhat, y):\n",
    "    err = (yhat - y) ** 2\n",
    "    ph = torch.sqrt(err.mean(dim=0))  # [H]\n",
    "    overall = torch.sqrt(err.mean())  # scalar\n",
    "    return ph, overall\n",
    "\n",
    "def skill_vs_persistence(yhat, y, y_last):\n",
    "    base = persistence_baseline(y_last, y.shape[1])\n",
    "    ph_m, overall_m = per_horizon_rmse(yhat, y)\n",
    "    ph_b, overall_b = per_horizon_rmse(base, y)\n",
    "    ph_skill = 1.0 - (ph_m / (ph_b + 1e-12))\n",
    "    overall_skill = 1.0 - (overall_m / (overall_b + 1e-12))\n",
    "    return ph_skill, overall_skill, ph_m, overall_m, ph_b, overall_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63aec9",
   "metadata": {},
   "source": [
    "## Training Loop (Huber + AdamW + OneCycle + Early Stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bd85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(model, loaders, device, epochs=80, max_lr=3e-3, clip=1.0, patience=8):\n",
    "    train_loader, val_loader = loaders\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=1e-2, betas=(0.9, 0.999))\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        opt, max_lr=max_lr, epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
    "        pct_start=0.1, div_factor=25.0, final_div_factor=1e4, three_phase=False\n",
    "    )\n",
    "    criterion = torch.nn.HuberLoss(delta=1.0, reduction='mean')\n",
    "\n",
    "    best_val = float('inf')\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        run_loss = 0.0\n",
    "        t0 = time.time()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            yhat = model(xb)\n",
    "            loss = criterion(yhat, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "            run_loss += loss.item() * xb.size(0)\n",
    "        train_loss = run_loss / (len(train_loader.dataset))\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        vsum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device); yb = yb.to(device)\n",
    "                yhat = model(xb)\n",
    "                vsum += torch.sqrt(((yhat - yb) ** 2).mean()).item() * xb.size(0)\n",
    "        val_rmse = vsum / len(val_loader.dataset)\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        print(f\"Epoch {ep:03d}/{epochs} | train_loss={train_loss:.5f} | val_RMSE={val_rmse:.5f} | dt={dt:.1f}s\")\n",
    "\n",
    "        if val_rmse < best_val - 1e-6:\n",
    "            best_val = val_rmse\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(f\"Early stopping at epoch {ep} (no improvement for {patience} epochs)\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return best_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783e042",
   "metadata": {},
   "source": [
    "## Configuration (edit me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6176ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === EDIT THESE ===\n",
    "CFG = {\n",
    "    \"npz\": \"../data/processed/sav_0927_v3.npz\",  # <-- set your NPZ path\n",
    "    \"horizon\": 24,     # H\n",
    "    \"lookback\": 168,   # L\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 80,\n",
    "    \"lr\": 3e-3,\n",
    "    \"channels\": 64,    # C\n",
    "    \"kernel\": 5,       # k\n",
    "    \"dropout\": 0.1,\n",
    "    \"dilations\": \"1,2,4,8,16,32\",\n",
    "    \"seed\": 42,\n",
    "    \"save\": \"best_model.pt\",\n",
    "    \"auto_tune\": False,\n",
    "    \"num_workers\": 2,\n",
    "}\n",
    "print(json.dumps(CFG, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba3a14",
   "metadata": {},
   "source": [
    "## Load Data, Trim to L/H, Normalize (train μ/σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c96b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_seed(CFG[\"seed\"])\n",
    "device = pick_device()\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load data\n",
    "Xtr, ytr, Xva, yva, Xte, yte = load_npz(CFG[\"npz\"])\n",
    "\n",
    "# Basic shape checks / trims to desired L,H if needed\n",
    "L, H = CFG[\"lookback\"], CFG[\"horizon\"]\n",
    "if Xtr.shape[1] != L:\n",
    "    if Xtr.shape[1] > L:\n",
    "        Xtr = Xtr[:, -L:, :]; Xva = Xva[:, -L:, :]; Xte = Xte[:, -L:, :]\n",
    "        print(f\"Trimmed lookback to last {L} steps.\")\n",
    "    else:\n",
    "        raise ValueError(f\"X lookback {Xtr.shape[1]} < desired L={L}.\")\n",
    "if ytr.shape[1] != H:\n",
    "    if ytr.shape[1] > H:\n",
    "        ytr = ytr[:, :H]; yva = yva[:, :H]; yte = yte[:, :H]\n",
    "        print(f\"Trimmed horizon to first {H} steps.\")\n",
    "    else:\n",
    "        raise ValueError(f\"y horizon {ytr.shape[1]} < desired H={H}.\")\n",
    "\n",
    "# Z-score per feature using TRAIN only\n",
    "mu, sd = zscore_fit(Xtr)\n",
    "Xtr = zscore_apply(Xtr, mu, sd)\n",
    "Xva = zscore_apply(Xva, mu, sd)\n",
    "Xte = zscore_apply(Xte, mu, sd)\n",
    "\n",
    "# Optional auto-tune\n",
    "if CFG[\"auto_tune\"]:\n",
    "    CFG = autotune_hparams(CFG, device)\n",
    "\n",
    "print(f\"[Config] bs={CFG['batch_size']}  epochs={CFG['epochs']}  C={CFG['channels']}  \"\n",
    "      f\"workers={CFG['num_workers']}  lr={CFG['lr']:.2e}\")\n",
    "\n",
    "# Dataloaders\n",
    "train_ds = TSWindowDataset(Xtr, ytr)\n",
    "val_ds   = TSWindowDataset(Xva, yva)\n",
    "test_ds  = TSWindowDataset(Xte, yte)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CFG[\"batch_size\"], shuffle=True, drop_last=False,\n",
    "                          num_workers=CFG[\"num_workers\"], pin_memory=False)\n",
    "val_loader   = DataLoader(val_ds, batch_size=CFG[\"batch_size\"], shuffle=False, drop_last=False,\n",
    "                          num_workers=CFG[\"num_workers\"], pin_memory=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=CFG[\"batch_size\"], shuffle=False, drop_last=False,\n",
    "                          num_workers=CFG[\"num_workers\"], pin_memory=False)\n",
    "F = Xtr.shape[-1]\n",
    "print(f\"Dataset sizes — train: {len(train_ds)}, val: {len(val_ds)}, test: {len(test_ds)}; F={F}, L={L}, H={H}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1238f90",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d8762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dilations = tuple(int(x) for x in CFG[\"dilations\"].split(\",\") if x.strip())\n",
    "model = DilatedCausalCNN(in_feats=F, C=CFG[\"channels\"], k=CFG[\"kernel\"], dilations=dilations,\n",
    "                         horizon=H, dropout=CFG[\"dropout\"]).to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model params: {n_params:,} | F={F}, L={L}, H={H}, dilations={dilations}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e522404",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7138f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_val = train_loop(\n",
    "    model, (train_loader, val_loader), device,\n",
    "    epochs=CFG[\"epochs\"], max_lr=CFG[\"lr\"], clip=1.0, patience=8\n",
    ")\n",
    "print(f\"Best val RMSE: {best_val:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7eaa51",
   "metadata": {},
   "source": [
    "## Save Best Model & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save({\"state_dict\": model.state_dict(),\n",
    "            \"mu\": mu, \"sd\": sd,\n",
    "            \"config\": CFG}, CFG[\"save\"])\n",
    "print(f\"Saved: {CFG['save']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef47573",
   "metadata": {},
   "source": [
    "## Evaluate on Test: Per-Horizon RMSE & Skill vs Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9645b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "yhats, ys, lasts = [], [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device); yb = yb.to(device)\n",
    "        yhat = model(xb)\n",
    "        yhats.append(yhat.cpu()); ys.append(yb.cpu())\n",
    "\n",
    "        # Baseline: using last observed 'temp' proxy\n",
    "        # NOTE: adjust this index if your 'temp' feature isn't the last feature!\n",
    "        temp_proxy = xb[:, -1, -1].detach().cpu()  # [B]\n",
    "        lasts.append(temp_proxy)\n",
    "\n",
    "yhat = torch.cat(yhats, dim=0)  # [N,H]\n",
    "y    = torch.cat(ys, dim=0)     # [N,H]\n",
    "last = torch.cat(lasts, dim=0)  # [N]\n",
    "\n",
    "ph_skill, overall_skill, ph_rmse_m, overall_rmse_m, ph_rmse_b, overall_rmse_b = skill_vs_persistence(yhat, y, last)\n",
    "\n",
    "print(\"\\n=== Test Metrics ===\")\n",
    "print(f\"Overall RMSE (model):       {overall_rmse_m.item():.4f}\")\n",
    "print(f\"Overall RMSE (persistence): {overall_rmse_b.item():.4f}\")\n",
    "print(f\"Overall Skill vs persist:   {overall_skill.item():.4f}\")\n",
    "print(\"\\nPer-horizon (t+1..t+H):\")\n",
    "for h in range(H):\n",
    "    print(f\"h+{h+1:02d}: RMSE_model={ph_rmse_m[h].item():.4f}  \"\n",
    "          f\"RMSE_persist={ph_rmse_b[h].item():.4f}  Skill={ph_skill[h].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad6ac7",
   "metadata": {},
   "source": [
    "\n",
    "## Tips for Iteration\n",
    "- **Swap loss**: try `nn.MSELoss()` or quantile loss heads.\n",
    "- **Alternate schedules**: CosineAnnealingLR or ReduceLROnPlateau.\n",
    "- **Change baseline**: persistence on other feature(s) or climatology.\n",
    "- **Diagnostics**: plot learning rates, gradient norms, or per-horizon charts.\n",
    "- **Stability**: try smaller `lr`, change `channels`, or increase patience.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}